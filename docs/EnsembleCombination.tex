\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{bbm}
\usepackage{xurl}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{breakurl}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue}

\usepackage{verbatim}
%% R
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

%% Reduce Bibliography space
\usepackage{enumitem}
\bibpunct{(}{)}{;}{a}{,}{,}

\baselineskip = 7 mm
\parskip = 2.5 mm


\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\begin{center}
{\bf\Large Ensembles and combinations: \\using multiple models to improve forecasts}
\end{center}


\bigskip


% \newpage
\spacingset{1.5} 


An increasing size of the toolbox of forecasting methods is available for decision makers. These methods, including statistical and econometric models, machine learning algorithms, and even judgemental forecasting \citep[see an encyclopedic overview by][]{petropoulos2020forecasting}, have their own specialities and are developed under different model specifications with assumptions on the Data Generation Process (DGP) or the associated error distributions. Given a pool of forecasting methods, how to best exploit information in the individual forecasts obtained from these methods?

Several studies in the forecasting literature are devoted to identifying a single 'best model' for a given time series. Given a family of models, information criteria, such as the Akaike Information Criterion \citep[AIC,][]{akaike1998information} and the Bayesian Information Criterion \citep[BIC,][]{schwarz1978estimating}, are commonly used for model selection \citep[e.g.,][]{qi2001investigation,billah2005empirical,yang2005can}. More generally, cross-validation, in its various forms, such as the hold-out approach and the out-of-sample rolling scheme, has been used successfully to select the best forecast when multiple model families or model-free forecasts are considered \citep[e.g.,][]{kohavi1995study,poler2011forecasting,fildes2015simple,inoue2017rolling,talagala2018meta}. However, different criteria may lead to different results of forecast selections. \cite{Kourentzes2019-na} argued that model selection is a challenging task for two reasons: the sample, parameter and model uncertainty associated with identifying a single best forecast, and the ill-defined best forecast.

Given these challenges, alternatively \cite{Bates1969-yj} have suggested combining multiple forecasts. The idea of combining forecasts is derived from the simple portfolio diversification argument \citep{Timmermann2006-en}, which is a risk management strategy with an obvious intuition: do not put all eggs into one basket. Even though slightly earlier articles have provided empirical justification of the superiority of forecast combinations over individual forecasts \citep[e.g.,][]{Barnard1963-xa,crane1967two}, the work by \cite{Bates1969-yj} is often considered to be the seminal article on forecast combinations as they developed a general analysis and further explored more possibilities for forecast combinations by extending a simple average to a weighted combination. Furthermore, the idea of combining forecasts is also widely used in machine learning, referred to as forecast ensembles. Similar to combination, the ensemble is a machine learning paradigm using multiple models to solve the same problem. It is difficult to trace the beginning of the history of ensemble forecasting. However, it is clear that ensemble techniques have become a hot topic in various fields, especially weather forecasting \cite[see an overview by][]{Leutbecher2008-mc}, since the 1990s. \cite{Lewis2005-hu} provided a genealogy to depict the scientific roots of ensemble forecasting from several fundamental lines of research. 

There are nearly five decades of empirical and theoretical investigations support that combining multiple forecasts often achieves improved forecasting performance on average than selecting a single individual forecast. Important early contributions in this area were summarized by \cite{Granger1989-gv}, \cite{Clemen1989-fb}, \cite{Palm1992-im}, and \cite{Timmermann2006-en}. \cite{Clemen1989-fb} surveyed over two hundred statistical literature on forecast combinations and provided a primary conclusion that forecasting accuracy can be substantially improved by combining multiple forecasts. \cite{Timmermann2006-en} summarized the benefits of forecast combinations into the fact that individual forecasts are obtained based on heterogeneous information sets, may be very differently affected by structural breaks and subject to misspecification bias of unknown form. They further concluded that forecast combinations are beneficial due to diversification gains. More recently, \cite{Atiya2020-ge} illustrated graphically why forecast combinations are superior.


\section{Simple Combinations}

Considerable literature has accumulated over the years regarding the way in which individual forecasts are combined. A unanimous conclusion is that simple combination schemes are hard to beat \citep{Clemen1989-fb,Stock2004-rq,Lichtendahl2020-ut}. More specifically, simple combination rules which ignore past information regarding the precision of individual forecasts and correlations between forecast errors work reasonably well relative to more sophisticated combination schemes \citep[see][]{Clemen1989-fb}. \cite{Lichtendahl2020-ut} attributed this phenomenon to a lower risk of simple combination methods resulting in bad forecasts than more refined combination methods. \cite{Timmermann2006-en} concisely summarized the reasons for the success of simple combinations by the importance of parameter estimation error---that is, simple combination schemes do not require estimating parameters such as combination weights based on forecast errors, thus avoiding parameter estimation error that often exists in the weighted combination.

The vast majority of studies on combining multiple models has dealt with point forecasting, even though point forecasts generally provide insufficient information for decision making. The simple average of forecasts based on equal weights is the most widely used simple combination rule \citep[see][]{Bunn1985-vo,Clemen1986-pd,Stock2003-sp}. \cite{Makridakis1982-hb} reported the results of M forecasting competition and found that a simple average outperformed all individual methods. \cite{Clemen1989-fb} provided a review and annotated bibliography of the early work on the combination of forecasts, and then addressed the issue that the arithmetic means often dominate more refined forecast combinations. \cite{Makridakis1983-hg} shown that combining forecasts using simple average reduces the variability of accuracy and hence the risk associated with selecting the best forecast. \cite{Palm1992-im} concisely summarized the advantages of adopting a simple average into three points: (i) combination weights are equal and do not have to be estimated, (ii) a simple average significantly reduces variance and bias by averaging out individual bias in many cases, and (iii) a simple average should be considered when the uncertainty of weight estimation is taken into account. Furthermore, \cite{Timmermann2006-en} pointed out that the good average performance of the simple average depends strongly on model instability and the ratio of forecast error variances associated with different forecasting models.  

More attention has been given to other options, including the median as well as trimmed means \citep[e.g.,][]{Chan1999-io,Stock2004-rq,Genre2013-ut,Jose2014-uh,Grushka-Cockayne2017-dj}, due to their robustness in the sense of being less affected by extreme forecasts than a simple average \citep{Lichtendahl2020-ut}. \cite{McNees1992-qc} found no significant difference between the mean and the trimmed mean, while the results of \cite{Stock2004-rq} support the mean. \cite{Jose2008-vm} studied the forecasting performance of the mean and median, as well as the trimmed and Winsorized means. Their results suggested that the trimmed and Winsorized means are appealing because of their simplicity and robust performance. \cite{Kourentzes2014-hs} compared empirically the mean, mode and median combination operators based on kernel density estimation, and found that the three operators deal with outlying extreme values differently, with the mean being the most sensitive and the mode operator the least. Based on these experimental results, they recommended further investigation of the use of the mode and median operators, which have been largely overlooked in relevant literature.

Compared to various refined combination approaches and advanced machine learning algorithms, simple combinations seem to be outdated and uncompetitive in the big data era. However, the results from the recent M4 competition \citep{Makridakis2020-hu} show that simple combinations can achieve fairly good forecasting performance and still be competitive. Specifically, a simple equal-weights combination achieved the third best performance for yearly time series \citep{Shaub2019-on} and a median combination of four models achieved sixth place for the point forecasts \citep{Petropoulos2020-fp}. Therefore, simple combination rules provide a tough benchmark to measure the effectiveness of the newly proposed weight estimation algorithms \citep[e.g.,][]{Makridakis2000-he,Stock2004-rq,Makridakis2020-hu,Montero-Manso2020-tq,Kang2020-rl,Wang2021-un}.


\section{Combination Weighting Schemes}

\subsection{Regression-based combination}

\subsection{Relative performance weights}
%forecast error
%information criterion

\subsection{Criterion-based combination}

\subsection{Combining by learning}
%feature; diversity; similarity

\subsection{Time-varying combination}

\subsection{Nonlinear combination}


\section{Forecast Combination Puzzle}
% About "forecast combination puzzle"
% This empirical fact has been first stated as the ``forecast combination puzzle" in \cite{Stock2004-rq}. Furthermore, some theoretical explanation of the forecast combination puzzle are offered in later studies \citep[see][]{Smith2009-wd,Claeskens2016-pv}.
% 1. This puzzle was first noted by Clemen (1989), and was formally named the ‘‘forecast combination puzzle’’ by Stock and Watson (2004).
% 2. In Makridakis et al., (1982, 1983), a simple average of six methods tended to be slightly more accurate than a weighted average of the same methods.


\section{Probabilistic Forecast Combinations}
% About "simple probabilistic forecast combination"
% 1. There has been a growing interest in using combination methods for probabilistic forecasting. Probabilistic forecasts differ from point forecasts in the sense that there is an associated probability related to the reported values, providing more comprehensive information about uncertainties of the future load than single-valued forecasts. 
% 2. histories (theory and empirical work of simple combination)
%% References:
%% - Is It Better to Average Probabilities or Quantiles?
%% - The opinion pool (and its citations)
%% - Combining probability distributions from experts in risk analysis
%% - Combining probability distributions: A critique and an annotated bibliography
% 3. three types and some basic combination methods
%% References:
%% - (interval)Combining Prediction Intervals in the M4 Competition
%% - Combining Interval Forecasts.
%% - (quantile)Combining Probabilistic Load Forecasts




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% References %%%%%%%%%%%%%%
\bibliographystyle{agsm}
\bibliography{../progress/paper-list.bib}

\end{document}
