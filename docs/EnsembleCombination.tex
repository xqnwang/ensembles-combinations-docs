\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{bbm}
\usepackage{xurl}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{breakurl}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue}

\usepackage{verbatim}
%% R
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

%% Cite options
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}

%% Reduce Bibliography space
\usepackage{enumitem}
\bibpunct{(}{)}{;}{a}{,}{,}

\baselineskip = 7 mm
\parskip = 2.5 mm

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{0ex}%
   {-3.25ex plus -1ex minus -0.2ex}%
   {1.5ex plus 0.2ex}%
   {\normalfont\normalsize\bfseries}}
\makeatother

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\begin{center}
{\bf\Large Ensembles and combinations: \\using multiple models to improve forecasts}
\end{center}


\bigskip


% \newpage
\spacingset{1.5} 


An increasing size of the toolbox of forecasting methods is available for decision makers. These methods, including statistical and econometric models, machine learning algorithms, and even judgemental forecasting \citep[see an encyclopedic overview by][]{petropoulos2020forecasting}, have their own specialities and are developed under different model specifications with assumptions on the Data Generation Process (DGP) or the associated error distributions. Given a pool of forecasting methods, how to best exploit information in the individual forecasts obtained from these methods?

Several studies in the forecasting literature are devoted to identifying a single `best model' for a given time series. Given a family of models, information criteria, such as the Akaike Information Criterion \citep[AIC,][]{akaike1998information} and the Bayesian Information Criterion \citep[BIC,][]{schwarz1978estimating}, are commonly used for model selection \citep[e.g.,][]{qi2001investigation,billah2005empirical,yang2005can}. More generally, cross-validation, in its various forms, such as the hold-out approach and the out-of-sample rolling scheme, has been used successfully to select the best forecast when multiple model families or model-free forecasts are considered \citep[e.g.,][]{kohavi1995study,poler2011forecasting,fildes2015simple,inoue2017rolling,talagala2018meta}. However, different criteria may lead to different results of forecast selections. \cite{Kourentzes2019-na} argued that model selection is a challenging task for two reasons: the sample, parameter and model uncertainty associated with identifying a single best forecast, and the ill-defined best forecast.

{\color{red} (Modified to use multiple models instead of combination $\downarrow$.)}
% refer to paper (ON COMBINING FORECASTS: SOME EXTENSIONS AND RESULTS; Pooling of forecasts; Kolassa2011-ai) for three reasons that could lead someone to using multiple models.

Given these challenges, alternatively \cite{Bates1969-yj} have suggested combining multiple forecasts. The idea of combining forecasts is derived from the simple portfolio diversification argument \citep{Timmermann2006-en}, which is a risk management strategy with an obvious intuition: do not put all eggs into one basket. Even though slightly earlier articles have provided empirical justification of the superiority of forecast combinations over individual forecasts \citep[e.g.,][]{Barnard1963-xa,crane1967two}, the work by \cite{Bates1969-yj} is often considered to be the seminal article on forecast combinations as they developed a general analysis and further explored more possibilities for forecast combinations by extending a simple average to a weighted combination. Furthermore, the idea of combining forecasts is also widely used in machine learning, referred to as forecast ensembles. Similar to combination, the ensemble is a machine learning paradigm using multiple models to solve the same problem. It is difficult to trace the beginning of the history of ensemble forecasting. However, it is clear that ensemble techniques have become a hot topic in various fields, especially weather forecasting \cite[see an overview by][]{Leutbecher2008-mc}, since the 1990s. \cite{Lewis2005-hu} provided a genealogy to depict the scientific roots of ensemble forecasting from several fundamental lines of research. 

There are nearly five decades of empirical and theoretical investigations support that combining multiple forecasts often achieves improved forecasting performance on average than selecting a single individual forecast. Important early contributions in this area were summarized by \cite{Granger1989-gv}, \cite{Clemen1989-fb}, \cite{Palm1992-im}, and \cite{Timmermann2006-en}. \cite{Clemen1989-fb} surveyed over two hundred statistical literature on forecast combinations and provided a primary conclusion that forecasting accuracy can be substantially improved by combining multiple forecasts. \cite{Timmermann2006-en} summarized the benefits of forecast combinations into the fact that individual forecasts are obtained based on heterogeneous information sets, may be very differently affected by structural breaks and subject to misspecification bias of unknown form. They further concluded that forecast combinations are beneficial due to diversification gains. More recently, \cite{Atiya2020-ge} illustrated graphically why forecast combinations are superior.

\section{Different ways of using multiple models}
\begin{itemize}
\item Combinations: A (usually linear) combination of forecasts from multiple models are used for one series. This includes combining point forecasts, quantile forecasts or full distributional forecasts. It covers simple averaging, weighted averaging, and sometimes combinations based on ML algorithms. e.g., FFORMA and related methods.
\item Ensembles: Although ``ensembles" has been used in different ways in different literatures, we will use ``ensemble" to mean a mixture of the forecast distributions from multiple models. In many ways this is simpler than combinations as the relationship between the methods can be ignored. Need to discuss when they are equivalent.
\item Boosting: Multiple models used for one series in sequence. Equivalent to hybrid forecasting where residuals from one method are modelled using a different method.
\item Bagging: One or more models applied to multiple similar series, and then a combination or ensemble is taken. Bagging requires a method for generating multiple series. Some possibilities are STL-ETS and GRATIS.
\item Stacking.
\end{itemize}

Simple example to illustrate differences. Suppose we have one series and two methods: an ARIMA model and a CNN.

\begin{itemize}
\item A combination would apply both to the same series and average the results. Unless we are only interested in point forecasting, the averaging would need to take account of the correlation between the forecast errors.
\item An ensemble would apply both to the same series and generate forecast distributions from each. These would then be mixed (possibly with weighting) to form the final forecast distribution.
\item Boosting would apply the ARIMA model to the series, and then apply the CNN to the residuals. The final forecasts would be the forecasts from the ARIMA model plus the forecasts from the CNN.
\item Bagging would generate multiple series like the series of interest, and apply one of the methods to all the generated series. These could then be combined, or ensembled.
\end{itemize}

\section{Point forecast combinations}

\subsection{Simple combinations}

Considerable literature has accumulated over the years regarding the way in which individual forecasts are combined. A unanimous conclusion is that simple combination schemes are hard to beat \citep{Clemen1989-fb,Stock2004-rq,Lichtendahl2020-ut}. More specifically, simple combination rules which ignore past information regarding the precision of individual forecasts and correlations between forecast errors work reasonably well relative to more sophisticated combination schemes, as noted in \citeapos{Clemen1989-fb} survey. \cite{Lichtendahl2020-ut} attributed this phenomenon to a lower risk of simple combination methods resulting in bad forecasts than more refined combination methods. \cite{Timmermann2006-en} concisely summarized the reasons for the success of simple combinations by the importance of parameter estimation error---that is, simple combination schemes do not require estimating parameters such as combination weights based on forecast errors, thus avoiding parameter estimation error that often exists in the weighted combination.

The vast majority of studies on combining multiple models has dealt with point forecasting, even though point forecasts generally provide insufficient information for decision making. The simple average of forecasts based on equal weights stands out as the most popular and surprisingly robust combination rule \citep[see][]{Bunn1985-vo,Clemen1986-pd,Stock2003-sp,Genre2013-ut}. \cite{Makridakis1982-hb} reported the results of M-competition, a forecasting competition involving $1001$ economic time series, and found that the simple average outperformed the individual techniques. \cite{Clemen1989-fb} provided an extensive bibliographical review of the early work on the combination of forecasts, and then addressed the issue that the arithmetic means often dominate more refined forecast combinations. \cite{Makridakis1983-hg} concluded empirically that the accuracy of combined forecasts is improved and the variability associated with the choice of methods is reduced, as the number of individual methods included in a simple average increases. \cite{Palm1992-im} concisely summarized the advantages of adopting a simple average into three points: (i) combination weights are equal and do not have to be estimated, (ii) a simple average significantly reduces variance and bias by averaging out individual bias in many cases, and (iii) a simple average should be considered when the uncertainty of weight estimation is taken into account. Furthermore, \cite{Timmermann2006-en} pointed out that the good average performance of the simple average depends strongly on model instability and the ratio of forecast error variances associated with different forecasting models.

More attention has been given to other options, including the median and mode, as well as trimmed means \citep[e.g.,][]{Chan1999-io,Stock2004-rq,Genre2013-ut,Jose2014-uh,Grushka-Cockayne2017-dj}, due to their robustness in the sense of being less affected by extreme forecasts than a simple average \citep{Lichtendahl2020-ut}. There is little consensus in the literature as to whether the mean or the median of individual forecasts performs better in terms of point forecasting \citep{Kolassa2011-ai}. Specifically, \cite{McNees1992-qc} found no significant difference between the mean and the median, while the results of \cite{Stock2004-rq} supported the mean and \cite{Agnew1985-dj} recommended the median. \cite{Jose2008-vm} studied the forecasting performance of the mean and median, as well as the trimmed and winsorized means. Their results suggested that the trimmed and winsorized means are appealing because of their simplicity and robust performance. \cite{Kourentzes2014-hs} compared empirically the mean, mode and median combination operators based on kernel density estimation, and found that the three operators deal with outlying extreme values differently, with the mean being the most sensitive and the mode operator the least. Based on these experimental results, they recommended further investigation of the use of the mode and median operators, which have been largely overlooked in relevant literature.

Compared to various refined combination approaches and advanced machine learning algorithms, simple combinations seem to be outdated and uncompetitive in the big data era. However, the results from the recent M4 competition \citep{Makridakis2020-hu} showed that simple combinations can achieve fairly good forecasting performance and still be competitive. Specifically, a simple equal-weights combination achieved the third best performance for yearly time series \citep{Shaub2019-on} and a median combination of four models achieved sixth place for the point forecasts \citep{Petropoulos2020-fp}. \cite{Genre2013-ut} encompassed a variety of combination methods in the case of forecasting GDP growth and the unemployment rate. They found that the simple average sets a high benchmark, with few of the combination schemes outperforming it. Therefore, simple combination rules have been consistently the choice of many researchers and provide a tough benchmark to measure the effectiveness of the newly proposed weight estimation algorithms \citep[e.g.,][]{Makridakis2000-he,Stock2004-rq,Makridakis2020-hu,Montero-Manso2020-tq,Kang2020-rl,Wang2021-un}.


\subsection{Combination weighting schemes}

% The success of combination highly depends on how well the combination weights can be determined \citep{De_Menezes2000-vd}, as well as the individual forecasts that are combined \citep[see more from][]{Kourentzes2019-na}.

% The shape of the combined forecast error distribution \citep{De_Gooijer2006-eg}.

Though the combined forecasts formed by simple combination rules are acceptable for illustrative and concise purposes, the accumulated evidence of the forecasting literature suggests assigning greater weights to the individual forecasts which contain lower errors. The issue to be addressed is how to best weight the different forecasts used for combination. The general point forecast combination problem can be defined as seeking a one-dimensional aggregator that reduces the information up to time $t$ in an $N$-vector of $h$-step-ahead forecasts, $\hat{\mathbf{y}}_{t+h|t}=\left(\hat{y}_{t+h|t, 1}, \hat{y}_{t+h|t, 2}, \ldots, \hat{y}_{t+h|t, N}\right)^{\prime}$, to a single combined $h$-step-ahead forecast $\tilde{y}_{t+h|t}=C\left(\hat{\mathbf{y}}_{t+h|t} ; \boldsymbol{w}_{t+h|t}\right)$, where $\boldsymbol{w}_{t+h|t}$ is an $N$-vector of combination weights. The general class of combination methods represented by the mapping, $C$, from $\hat{\mathbf{y}}_{t+h|t}$ to $y_{t+h}$, comprises linear, non-linear, and time-varying combinations. Below we discuss in detail the use of various combination weight schemes to determine combination weights.

\subsubsection{Linear combinations}

Typically, the combined forecast is commonly constructed as a linear combination of the individual forecasts. To this end a combined forecast of the linear form can be written as
\begin{align}
\label{eq:linear-combinations}
\tilde{y}_{t+h|t}=\boldsymbol{w}_{t+h|t}^{\prime} \hat{\mathbf{y}}_{t+h|t},
\end{align}
where $\boldsymbol{w}_{t+h|t}=\left(w_{t+h|t, 1}, \ldots, w_{t+h|t, N}\right)^{\prime}$ is an $N$-vector of linear combination weights assigned to $N$ individual forecasts.

\paragraph{Optimal weights}

The seminal work of \cite{Bates1969-yj} proposed a method to find `optimal' weights by minimizing the variance of the combined forecast error, and discussed only the combination of pairs of forecasts. \cite{Newbold1974-lp} then extended the method to the combination of several forecasts. Specifically, assuming that individual forecasts are unbiased and their variance of errors is consistent over time, the combined forecast obtained by a linear combination will also be unbiased. Differentiating with respect to $\boldsymbol{w}_{t+h|t}$ and solving the first order condition, the variance of the combined forecast error is minimized by taking
\begin{align}
\label{eq:weight_opt}
\boldsymbol{w}_{t+h|t}^{opt}=\frac{\boldsymbol{\Sigma}_{t+h|t}^{-1}\mathbf{1}}{\mathbf{1}^{\prime} \boldsymbol{\Sigma}_{t+h|t}^{-1} \mathbf{1}},
\end{align}
where $\boldsymbol{\Sigma}_{t+h|t}$ is the $N \times N$ covariance matrix of the lead $h$ forecast errors and $\mathbf{1}$ is the $N$-dimensional unit vector. Unfortunately, in practice, the elements of the covariance matrix $\boldsymbol{\Sigma}_{t+h|t}$ are usually unknown and required to be properly estimated.

It follows that if $\boldsymbol{w}_{t+h|t}$ is determined by Equation~\eqref{eq:weight_opt}, one can find a combined forecast $\tilde{y}_{t+h|t}$ with no greater error variance than the minimum error variance of all individual forecasts. The fact was further demonstrated in detail in \cite{Timmermann2006-en} to illustrate the diversification gains offered by forecast combinations by simply considering the combination of two forecasts. Under Mean Squared Error (MSE) loss, \cite{Timmermann2006-en} characterized the general solution of the optimal linear combination weights given the joint Gaussian distribution of the outcome $y_{t+h}$ and forecasts $\hat{\mathbf{y}}_{t+h|t}$.

The loss assumed in \cite{Bates1969-yj} and \cite{Newbold1974-lp} is quadratic and symmetric in the forecast error from the linear combination. \cite{Elliott2004-dz} examined forecast combinations under more general loss functions that account for asymmetries, and forecast error distributions with skew. They demonstrated that the optimal combination weights in a combination strongly depend on the degree of asymmetry in the loss function and skews in the underlying forecast error distribution. Subsequently, \cite{Patton2007-zo} demonstrated that the properties of optimal forecasts established under MSE loss are not generally robust under more general assumptions about the loss function. The properties of optimal forecasts were also generalized to consider asymmetric loss and nonlinear DGP.

% Optimal weights (A weighted average of forecasts that minimizes the variance of the combined forecast error) -- Unknown covariance matrix -- Five practical methods (relative performance \& time-varying weights).

\paragraph{Regression approach}

The seminal work by \cite{Granger1984-jc} provided an important impetus for approximating the optimal weights under a linear regression framework. They recommended the strategy that the combination weights are estimated by ordinary least squares (OLS) in regression models having the vector of past observations as the response variable and the matrix of past forecasts as the explanatory variables. Three alternative approaches involving various possible restrictions are considered
\begin{align}
&y_{t+h}=\boldsymbol{w}_{h}^{\prime} \hat{\mathbf{y}}_{t+h|t}+\varepsilon_{t+h}, \quad s.t. \quad \boldsymbol{w}_{h}^{\prime}\mathbf{1}=1, \label{eq:weight_gr1}\\
&y_{t+h}=\boldsymbol{w}^{\prime} \hat{\mathbf{y}}_{t+h|t}+\varepsilon_{t+h}, \label{eq:weight_gr2}\\
&y_{t+h}=\omega_{0h}+\boldsymbol{w}^{\prime} \hat{\mathbf{y}}_{t+h|t}+\varepsilon_{t+h}. \label{eq:weight_gr3}
\end{align}
The constrained OLS estimation of the regression~\eqref{eq:weight_gr1} in which the constant is omitted and the weights are constrained to sum to one yields results identical to the optimal weights proposed by \cite{Bates1969-yj}. Furthermore, \cite{Granger1984-jc} suggested the unrestricted OLS regression~\eqref{eq:weight_gr3} which allows for a constant term and dose not impose the weights sum to one is superior to the popular optimal method regardless of whether the constituent forecasts are biased. However, \cite{De_Menezes2000-vd} put forward some consideration required when using the unrestricted regression, including the stationarity of the series being forecast, the possible presence of serial correlation in forecast errors \citep[see also][]{Diebold1988-sx,Edward_Coulson1993-db}, and the issue of multicollinearity.

More generalizations of the combination regressions have been considered in the literature. \cite{Diebold1988-sx} exploited the serial correlation in least squares framework by characterizing the combined forecast errors as the autoregressive moving average (ARMA) processes, leading to improved combined forecasts. \cite{Gunter1992-go} and \cite{Aksu1992-lb} provided an empirical analysis to compare the performance of various combination strategies, including the simple average, the unrestricted OLS regression, the restricted OLS regression where the weights are restricted to sum to unity, and the nonnegativity restricted OLS regression where the weights are constrained to be nonnegative. The results revealed that constraining weights to be nonnegative  is at least as robust and accurate as the simple average and yields superiority over other combinations based on regression framework. \cite{Conflitti2015-fq} addressed the problem of determining the optimal weights by imposing two restrictions that the weights should be nonnegative and sum to one, which turns out to be a special case of a lasso regression. \cite{Edward_Coulson1993-db} found that allowing a lagged dependent variable in forecast combination regressions can achieve improved performance. Instead of using the quadratic loss function, \cite{Nowotarski2014-ev} applied the absolute loss function in the unrestricted regression to yield the least absolute deviation regression which is more robust to outliers than OLS combinations.

The forecast combinations using changing weights are developed to consider various types of structual changes in the constituent forecasts in the relevant literature. For instance, \cite{Diebold1987-go} explored the possibilities for time-varying parameters in regression-based combination approaches. Both deterministic and stochastic time-varying parameters are considered in the linear regression framework. Specifically, the combination weights are described as deterministic nonlinear (polynomial) functions of time or allowed to involve random variation. \cite{Deutsch1994-ob} allowed the combination weights to evolve immediately or smoothly using switching regression models and smooth transition regression models.

Researchers have worked on dealing with a large number of forecasts in the regression framework to take advantages of many different models. \cite{Chan1999-io} examined a wide range of combination methods in a Monte Carlo experiment and a real-word dataset. Their results investigated the poor performance of OLS combinations when the number of forecasts to be combined is large and suggested alternative weight estimation methods, such as ridge regression and principal components forecast combination. \cite{Stock2004-rq} provided the details of principal component forecast combination, which entails forming a regression having the actual value as the response variable and the first few principal components reduced from several forecasts as the explanatory variables. This method reduces the number of weights that must be estimated in a regression framework, and frequently serves as a way to solve the multicollinearity problem which is likely to lead to unstable behavior in the estimated weights. The superiority of the principal components regression that involves dimension reduction techniques over OLS combinations was also supproted in \cite{Rapach2008-jh} and \cite{Poncela2011-vz}.

\paragraph{Performance-based weights}

Estimation errors in the optimal weights and a diverse set of regression-based weights tend to be particularly large due to difficulties in properly estimating the entire covariance matrix $\boldsymbol{\Sigma}_{t+h|t}$, especially in situations with the large number of forecasts at hand. Instead, \cite{Bates1969-yj} suggested weighting the constituent forecasts in inverse proportion to their historical performance, ignoring correlations across forecast errors. In follow-up studies, \cite{Newbold1974-lp} and \cite{Winkler1983-ra} generalized the issue in the sense of considering more time series, more forecasting models, and multiple forecast horizons. Their extensive results demonstrated that combinations  which take account of correlations perform poorly, and consequently reconfirmed \cite{Bates1969-yj} argument that correlations can be poorly estimated in practice and should be ignored in calculating combination weights.

Let $\mathbf{e}_{t+h|t}=\mathbf{1} y_{t+h}-\hat{\mathbf{y}}_{t+h|t}$ be the $N$-vector of $h$-period forecast errors from the individual models, the five procedures suggested in \cite{Bates1969-yj} for estimating the combination weights when $\boldsymbol{\Sigma}_{t+h|t}$ is unknown, extended to the general case are as follows:
\begin{align}
&w_{t+h|t, i}^{bg1}=\frac{\left( \sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, i}^{2} \right)^{-1}}{\sum_{j=1}^{N}\left(\sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, j}^{2}\right)^{-1}}. \label{eq:weight_bg1}\\
&\boldsymbol{w}_{t+h|t}^{bg2}=\frac{\hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1}\mathbf{1}}{\mathbf{1}^{\prime} \hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1} \mathbf{1}}, \quad \text{where} \quad (\hat{\boldsymbol{\Sigma}}_{t+h|t})_{i, j}=\nu^{-1} \sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, i} e_{\tau|\tau-h, j}. \label{eq:weight_bg2}\\
&w_{t+h|t, i}^{bg3}=\alpha \hat{w}_{t+h-1|t-1, i} + (1-\alpha) \frac{\left( \sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, i}^{2} \right)^{-1}}{\sum_{j=1}^{N}\left(\sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, j}^{2}\right)^{-1}}, \quad 0<\alpha<1. \label{eq:weight_bg3}\\
&w_{t+h|t, i}^{bg4}=\frac{\left( \sum_{\tau=1}^{t} \gamma^{\tau} e_{\tau|\tau-h, i}^{2} \right)^{-1}}{\sum_{j=1}^{N}\left(\sum_{\tau=1}^{t} \gamma^{\tau} e_{\tau|\tau-h, j}^{2}\right)^{-1}}, \quad \gamma \geq 1. \label{eq:weight_bg4}\\
&\boldsymbol{w}_{t+h|t}^{bg5}=\frac{\hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1}\mathbf{1}}{\mathbf{1}^{\prime} \hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1} \mathbf{1}}, \quad \text{where} \quad (\hat{\boldsymbol{\Sigma}}_{t+h|t})_{i, j}=\frac{\sum_{\tau=1}^{t} \gamma^{\tau} e_{\tau|\tau-h, i} e_{\tau|\tau-h, j}}{\sum_{\tau=1}^{t} \gamma^{\tau}} \quad \text{and} \quad \gamma \geq 1. \label{eq:weight_bg5}
\end{align}
These weighting schemes differ in the factors, as well as the choice of the parameters, $\nu$, $\alpha$, and $\gamma$. Correlations across forecast errors are either ignored by treating the covariance matrix $\boldsymbol{\Sigma}_{t+h|t}$ as a diagonal matrix or estimated using rolling windows or exponential discounting. Some estimation schemes \eqref{eq:weight_bg1}-\eqref{eq:weight_bg3} suggest computing or updating the relative performance of different models over rolling windows of the most recent $\nu$ observations, while others \eqref{eq:weight_bg4}-\eqref{eq:weight_bg5} base the weights on exponential discounting with higher values of $\gamma$ giving larger weights to recent observations. In consequence, these weighting schemes are well adapted to allow the non-stationary relationship between the individual forecasting procedures over time \citep{Newbold1974-lp}, which, however, tends to increase the variance of the parameter estimates and works quite poorly provided that the DGP is truly covariance stationary \citep{Timmermann2006-en}.

A broader set of combination weights based on the relative performance of individual forecasting techniques is developed and examined in a series of studies. \cite{Stock1998-np} generalized the rolling window scheme \eqref{eq:weight_bg1} in the sense that the weights on the individual forecasts are inversely proportional to the $k$th power of their MSE. The weights with $k=0$ correspond to assigning equal weights to all forecasts, while more weights are placed on the best performing models by considering $k \geq 1$. Other forms of forecast error measures, such as the RMSE (Root Mean Squared Error) and sMAPE (symmetric Mean Absolute Percentage Error), are also considered to develop the performance-based combination weights \citep[e.g.,][]{Nowotarski2014-ev,Pawlikowski2020-hm}. Besides, a weighting scheme with the weights depending inversely on the exponentially discounted errors is proposed by \cite{Stock2004-rq} as an upgraded version of the scheme \eqref{eq:weight_bg4}, and encompassed in the sequent studies \citep[e.g.,][]{Clark2010-jx,Genre2013-ut} to achieve gains from combining forecasts. The pseudo out-of-sample performance used in these weighting schemes is commonly computed based on rolling or recursive (expanding) windows \citep[e.g.,][]{Stock1998-np,Clark2010-jx,Genre2013-ut}. It is natural to use rolling windows in estimating the weights to deal with the structural change. But the window length should not be too short without the estimates of the weights becoming too noisy \citep{Baumeister2015-ft}.
% rolling windows (min(t-T+1, v)) \citep{Stock1998-np,Clark2010-jx,Genre2013-ut,Baumeister2015-ft,Pawlikowski2020-hm}
% recursive windows (over the past t-T+1 periods) \citep[expanding,][]{Stock1998-np,Stock2003-sp,Stock2004-rq,Clark2010-jx,Genre2013-ut,Nowotarski2014-ev,Baumeister2015-ft}

Compared to constructing the weights directly using historical forecast errors, a new form of combination that is more robust and less sensitive to outliers is introduced based on the `ranking' of models. Again this combination ignores correlations across forecast errors. The simplest and most commonly used method in the class is to use the median forecast as the output. \cite{Aiolfi2006-rh} constructed the weights proportional to the inverse of performance ranks (sorted according to increasing order of forecast errors), which were later used by \cite{Andrawis2011-kb} for tourism demand forecasting. Another weighting scheme that attaches a weight proportional to $\exp (\beta(N+1-i))$ to the $i$th ordered constituent model is adopted in \cite{Yao2008-or} and \cite{Donate2013-lq} to combine ANNs (Artificial Neural Networks), where $\beta$ is a scaling factor. However, as mentioned by \cite{Andrawis2011-kb}, this class of combination method still comes with the drawback of the discrete nature because it limits the weight to only a few possible levels.

\paragraph{Combinations based on information criteria}

Information criteria, such as AIC \citep[Akaike’s Information Criterion,]{Akaike1974-ya}, AICc \citep{Sugiura1978-xm}, and BIC \citep[Bayes Information Criterion,][]{Schwarz1978-cz}, are often advised to deal with model selection in forecasting. An alternative way proposed by \cite{Burnham2002-us} is to combine different models based on information criteria to mitigate the risk of selecting a single model.

One such common approach is using Akaike weights. Specifically, in light of the fact that AIC estimates the Kullback-Leibler distance \citep{Kullback1951-hl} between a model and the true DGP, differences in the AIC can be considered to weight different models, providing a measure of the evidence for each model relative to other constituent models. Given $N$ individual models, the Akaike weight of model $i$ can be derived by the following steps:
\begin{align}
&\Delta \mathrm{AIC}_{i}=\mathrm{AIC}_{i}-\min _{k \in \{1,2,\cdots,N\}} \mathrm{AIC}(k), \label{eq:diff_aic}\\
&w_{i}=\frac{\exp (-0.5 \Delta \mathrm{AIC}_{i})}{\sum_{k=1}^{N} \exp \left(-0.5 \Delta \mathrm{AIC}_{k}\right)}. \label{eq:weight_aic}
\end{align}
Akaike weights calculated in this manner can be interpreted as the probability that a given model performs best at approximating the unknown DGP, given the model set and data \citep{Kolassa2011-ai}. Similar weights from AICc and BIC can be derived analogously to Equation~\eqref{eq:diff_aic} and \eqref{eq:weight_aic}.

The outstanding performance of weighted combinations based on information criteria has been confirmed in several research. For instance, \cite{Kolassa2011-ai} used weights derived from AIC, AICc and BIC to combine exponential smoothing forecasts, and resulted in superior accuracy over selection by these information criteria. The similar strategy was adopted by \cite{Petropoulos2018-fw} to separately explore the benefits of bagging for time series forecasting. Furthermore, an empirical study by \cite{Petropoulos2018-ad} showed that a weighted combination based on AIC improves the performance of the statistical benchmark they use.

% the use of multiple temporal aggregation levels

\paragraph{Bayesian weights}

see review by \cite{Cheng2015-tp}.

\citep{Bunn1975-mg,Clemen1993-ey,Nowotarski2014-ev}.

%  Bayesian forecast combinations are being increasingly used in macroeconomics and finance to good effect.


\subsubsection{Nonlinear combinations}


\subsubsection{Combining by learning}
\begin{itemize}
 \item Stacking: introduce stacking as a combination/aggregation method rather than a generalization of many ensemble methods in ML.
 \item Regression-based combinations: a simple stacking.
 \item Meta-learning that only takes individual forecasts as input.
 \item Stacking extension: use other information in the meta-learner, such as feature and diversity (FFORMA).
\end{itemize}

\subsubsection{Which forecasts should be combined?}

forecast pooling / optimal forecast groups / model selection
\citep{Zhou2002-cg,Kourentzes2019-na}



\subsection{Forecast Combination Puzzle}
{\color{red} (why don't weights work?)}

\citep{Kang1986-kq,De_Menezes2000-vd,Genre2013-ut,Post2019-lv,Chan2018-jl,Lichtendahl2020-ut,Kourentzes2019-na}
% About "forecast combination puzzle"
% This empirical fact has been first stated as the ``forecast combination puzzle" in \cite{Stock2004-rq}. Furthermore, some theoretical explanation of the forecast combination puzzle are offered in later studies \citep[see][]{Smith2009-wd,Claeskens2016-pv}.
% 1. This puzzle was first noted by Clemen (1989), and was formally named the ‘‘forecast combination puzzle’’ by Stock and Watson (2004).
% 2. In Makridakis et al., (1982, 1983), a simple average of six methods tended to be slightly more accurate than a weighted average of the same methods.


\section{Probabilistic forecast combinations}
\begin{itemize}
\item Combining quantiles and prediction intervals.
\item Combining distributions as in fable.
\end{itemize}

\citep{Genest1990-sr,Nowotarski2015-xu,Conflitti2015-fq,Billio2013-sg}


% About "simple probabilistic forecast combination"
% 1. There has been a growing interest in using combination methods for probabilistic forecasting. Probabilistic forecasts differ from point forecasts in the sense that there is an associated probability related to the reported values, providing more comprehensive information about uncertainties of the future load than single-valued forecasts. 
% 2. histories (theory and empirical work of simple combination)
%% References:
%% - Is It Better to Average Probabilities or Quantiles?
%% - The opinion pool (and its citations)
%% - Combining probability distributions from experts in risk analysis
%% - Combining probability distributions: A critique and an annotated bibliography
% 3. three types and some basic combination methods
%% References:
%% - (interval)Combining Prediction Intervals in the M4 Competition
%% - Combining Interval Forecasts.
%% - (quantile)Combining Probabilistic Load Forecasts

\section{Probabilistic ensembles}
\begin{itemize}
\item Meteorological ensembles.
\item True ensembles in other areas (i.e., not papers that use the word "ensemble" but papers that use mixtures when forecasting).
\item When is an ensemble equivalent to combination?
\item When do point forecasts from an ensemble equal point forecasts from a combination?
\end{itemize}

\section{Boosting in forecasting}
\section{Bagging in forecasting}
\section{Stacking in forecasting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% References %%%%%%%%%%%%%%
\bibliographystyle{agsm}
\bibliography{../progress/paper-list.bib}

\end{document}
