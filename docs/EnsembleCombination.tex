\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{bbm}
\usepackage{xurl}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{breakurl}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue}

\usepackage{verbatim}
%% R
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

%% Cite options
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}

%% Reduce Bibliography space
\usepackage{enumitem}
\bibpunct{(}{)}{;}{a}{,}{,}

\baselineskip = 7 mm
\parskip = 2.5 mm

\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{0ex}%
   {-3.25ex plus -1ex minus -0.2ex}%
   {1.5ex plus 0.2ex}%
   {\normalfont\normalsize\bfseries}}
\makeatother

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\begin{center}
{\bf\Large Ensembles and combinations: \\using multiple models to improve forecasts}
\end{center}


\bigskip


% \newpage
\spacingset{1.5}


An increasing size of the toolbox of forecasting methods is available for decision makers. These methods, including statistical and econometric models, machine learning algorithms, and even judgemental forecasting \citep[see an encyclopedic overview by][]{petropoulos2020forecasting}, have their own specialities and are developed under different model specifications with assumptions on the Data Generation Process (DGP) or the associated error distributions. Given a pool of forecasting methods, how to best exploit information in the individual forecasts obtained from these methods?

Several studies in the forecasting literature are devoted to identifying a single `best model' for a given time series. Given a family of models, information criteria, such as the Akaike Information Criterion \citep[AIC,][]{Akaike1974-ya} and the Bayesian Information Criterion \citep[BIC,][]{Schwarz1978-cz}, are commonly used for model selection \citep[e.g.,][]{qi2001investigation,billah2005empirical,yang2005can}. More generally, cross-validation, in its various forms, such as the hold-out approach and the out-of-sample rolling scheme, has been used successfully to select the best forecast when multiple model families or model-free forecasts are considered \citep[e.g.,][]{kohavi1995study,poler2011forecasting,fildes2015simple,inoue2017rolling,talagala2018meta}. However, different criteria may lead to different results of forecast selections. \cite{Kourentzes2019-na} argued that model selection was a challenging task for two reasons: the sample, parameter and model uncertainty associated with identifying a single best forecast, and the ill-defined best forecast.

{\color{red} (Modified to use multiple models instead of combination $\downarrow$.)}
% refer to paper (ON COMBINING FORECASTS: SOME EXTENSIONS AND RESULTS; Pooling of forecasts; Kolassa2011-ai) for three reasons that could lead someone to using multiple models.

Given these challenges, alternatively \cite{Bates1969-yj} have suggested combining multiple forecasts. The idea of combining forecasts is derived from the simple portfolio diversification argument \citep{Timmermann2006-en}, which is a risk management strategy with an obvious intuition: do not put all eggs into one basket. Even though slightly earlier articles have provided empirical justification of the superiority of forecast combinations over individual forecasts \citep[e.g.,][]{Barnard1963-xa,crane1967two}, the work by \cite{Bates1969-yj} is often considered to be the seminal article on forecast combinations as they developed a general analysis and further explored more possibilities for forecast combinations by extending a simple average to a weighted combination. Furthermore, the idea of combining forecasts is also widely used in machine learning, referred to as forecast ensembles. Similar to combination, the ensemble is a machine learning paradigm using multiple models to solve the same problem. It is difficult to trace the beginning of the history of ensemble forecasting. However, it is clear that ensemble techniques have become a hot topic in various fields, especially weather forecasting \cite[see an overview by][]{Leutbecher2008-mc}, since the 1990s. \cite{Lewis2005-hu} provided a genealogy to depict the scientific roots of ensemble forecasting from several fundamental lines of research.

There are nearly five decades of empirical and theoretical investigations support that combining multiple forecasts often achieves improved forecasting performance on average than selecting a single individual forecast. Important early contributions in this area were summarized by \cite{Granger1989-gv}, \cite{Clemen1989-fb}, \cite{Palm1992-im}, and \cite{Timmermann2006-en}. \cite{Clemen1989-fb} surveyed over two hundred statistical literature on forecast combinations and provided a primary conclusion that forecasting accuracy could be substantially improved by combining multiple forecasts. \cite{Timmermann2006-en} attributed the superiority of forecast combinations over a single model to the fact that individual forecasts obtained based on heterogeneous information sets, may be very differently affected by structural breaks and subject to misspecification bias of unknown form. They further concluded that forecast combinations were beneficial due to diversification gains. More recently, \cite{Atiya2020-ge} illustrated graphically why forecast combinations were superior.

\section{Different ways of using multiple models}
\begin{itemize}
\item Combinations: A (usually linear) combination of forecasts from multiple models are used for one series. This includes combining point forecasts, quantile forecasts or full distributional forecasts. It covers simple averaging, weighted averaging, and sometimes combinations based on ML algorithms. e.g., FFORMA and related methods.
\item Ensembles: Although ``ensembles" has been used in different ways in different literatures, we will use ``ensemble" to mean a mixture of the forecast distributions from multiple models. In many ways this is simpler than combinations as the relationship between the methods can be ignored. Need to discuss when they are equivalent.
\item Boosting: Multiple models used for one series in sequence. Equivalent to hybrid forecasting where residuals from one method are modelled using a different method.
\item Bagging: One or more models applied to multiple similar series, and then a combination or ensemble is taken. Bagging requires a method for generating multiple series. Some possibilities are STL-ETS and GRATIS.
\item Stacking.
\end{itemize}

Simple example to illustrate differences. Suppose we have one series and two methods: an ARIMA model and a CNN.

\begin{itemize}
\item A combination would apply both to the same series and average the results. Unless we are only interested in point forecasting, the averaging would need to take account of the correlation between the forecast errors.
\item An ensemble would apply both to the same series and generate forecast distributions from each. These would then be mixed (possibly with weighting) to form the final forecast distribution.
\item Boosting would apply the ARIMA model to the series, and then apply the CNN to the residuals. The final forecasts would be the forecasts from the ARIMA model plus the forecasts from the CNN.
\item Bagging would generate multiple series like the series of interest, and apply one of the methods to all the generated series. These could then be combined, or ensembled.
\end{itemize}

\section{Point forecast combinations}

\subsection{Simple combinations}

Considerable literature has accumulated over the years regarding the way in which individual forecasts are combined. A unanimous conclusion is that simple combination schemes are hard to beat \citep{Clemen1989-fb,Fischer1999-kz,Stock2004-rq,Lichtendahl2020-ut}. More specifically, simple combination rules which ignore past information regarding the precision of individual forecasts and correlations between forecast errors work reasonably well relative to more sophisticated combination schemes, as noted in \citeapos{Clemen1989-fb} survey. \cite{Lichtendahl2020-ut} attributed this phenomenon to a lower risk of simple combination methods resulting in bad forecasts than more refined combination methods. \cite{Timmermann2006-en} concisely summarized the reasons for the success of simple combinations by the importance of parameter estimation error---simple combination schemes did not require estimating parameters such as combination weights based on forecast errors, thus avoiding parameter estimation error that often existed in weighted combinations.

The vast majority of studies on combining multiple models has dealt with point forecasting, even though point forecasts generally provide insufficient information for decision making. The simple average of forecasts based on equal weights stands out as the most popular and surprisingly robust combination rule \citep[see][]{Bunn1985-vo,Clemen1986-pd,Stock2003-sp,Genre2013-ut}. \cite{Makridakis1982-hb} reported the results of M-competition, a forecasting competition involving $1,001$ economic time series, and found that the simple average outperformed the individual techniques. \cite{Clemen1989-fb} provided an extensive bibliographical review of the early work on the combination of forecasts, and then addressed the issue that the arithmetic means often dominated more refined forecast combinations. \cite{Makridakis1983-hg} concluded empirically that the accuracy of combined forecasts was improved and the variability associated with the choice of methods was reduced, as the number of individual methods included in a simple average increased. \cite{Palm1992-im} concisely summarized the advantages of adopting a simple average into three points: (i) combination weights were equal and did not have to be estimated, (ii) a simple average significantly reduced variance and bias by averaging out individual bias in many cases, and (iii) a simple average should be considered when the uncertainty of weight estimation was taken into account. Furthermore, \cite{Timmermann2006-en} pointed out that the good average performance of the simple average depended strongly on model instability and the ratio of forecast error variances associated with different forecasting models.

More attention has been given to other options, including the median and mode, as well as trimmed means \citep[e.g.,][]{Chan1999-io,Stock2004-rq,Genre2013-ut,Jose2014-uh,Grushka-Cockayne2017-dj}, due to their robustness in the sense of being less sensitive to extreme forecasts than a simple average \citep{Lichtendahl2020-ut}. There is little consensus in the literature as to whether the mean or the median of individual forecasts performs better in terms of point forecasting \citep{Kolassa2011-ai}. Specifically, \cite{McNees1992-qc} found no significant difference between the mean and the median, while the results of \cite{Stock2004-rq} supported the mean and \cite{Agnew1985-dj} recommended the median. \cite{Jose2008-vm} studied the forecasting performance of the mean and median, as well as the trimmed and winsorized means. Their results suggested that the trimmed and winsorized means were appealing because of their simplicity and robust performance. \cite{Kourentzes2014-hs} compared empirically the mean, mode and median combination operators based on kernel density estimation, and found that the three operators dealt with outlying extreme values differently, with the mean being the most sensitive and the mode operator the least. Based on these experimental results, they recommended further investigation of the use of the mode and median operators, which had been largely overlooked in relevant literature.

Compared to various refined combination approaches and advanced machine learning algorithms, simple combinations seem to be outdated and uncompetitive in the big data era. However, the results from the recent M4 competition \citep{Makridakis2020-hu} showed that simple combinations could achieve fairly good forecasting performance and still be competitive. Specifically, a simple equal-weights combination achieved the third best performance for yearly time series \citep{Shaub2019-on} and a median combination of four models achieved sixth place for the point forecasts \citep{Petropoulos2020-fp}. \cite{Genre2013-ut} encompassed a variety of combination methods in the case of forecasting GDP growth and the unemployment rate. They found that the simple average set a high benchmark, with few of the combination schemes outperforming it. Therefore, simple combination rules have been consistently the choice of many researchers and provide a tough benchmark to measure the effectiveness of the newly proposed weight estimation algorithms \citep[e.g.,][]{Makridakis2000-he,Stock2004-rq,Makridakis2020-hu,Montero-Manso2020-tq,Kang2020-rl,Wang2021-un}. They have less computational burden, and can be implemented quickly.

Although simple combination schemes can be intuitively implemented, the success of combination still highly depends on the choice of the model pool. If all component models are established in a very similar way based on the same set of information, forecast combination makes no sense, and can not play its positive role in boosting forecasting performance. \cite{Mannes2014-dl} and \cite{Lichtendahl2020-ut} incorporated empirical analysis of simple combination methods, and emphasized two important issues with respect to the forecasting models being combined: one for accuracy (or expertise) and another for diversity. Including component models with low accuracy is not likely to improve the combined forecasts. In addition, a high degree of diversity among component models facilitates the achievement of the best possible forecasting accuracy from their simple combinations \citep{Thomson2019-al}. In conclusion, simple, easy-to-use combination rules can provide good, and robust forecasting performance, especially when considering issues such as accuracy, and diversity of the method pool used for combining.


\subsection{Combination weighting schemes}

% The success of combination highly depends on how well the combination weights can be determined \citep{De_Menezes2000-vd}, as well as the individual forecasts that are combined \citep[see more from][]{Kourentzes2019-na}.

Though the combined forecasts formed by simple combination rules are acceptable for illustrative and concise purposes, the accumulated evidence of the forecasting literature suggests assigning greater weights to the individual forecasts which contain lower errors. The issue to be addressed is how to best weight the different forecasts used for combination. The general point forecast combination problem can be defined as seeking a one-dimensional aggregator that reduces the information up to time $t$ in an $N$-vector of $h$-step-ahead forecasts, $\hat{\mathbf{y}}_{t+h|t}=\left(\hat{y}_{t+h|t, 1}, \hat{y}_{t+h|t, 2}, \ldots, \hat{y}_{t+h|t, N}\right)^{\prime}$, to a single combined $h$-step-ahead forecast $\tilde{y}_{t+h|t}=C\left(\hat{\mathbf{y}}_{t+h|t} ; \boldsymbol{w}_{t+h|t}\right)$, where $\boldsymbol{w}_{t+h|t}$ is an $N$-vector of combination weights. The general class of combination methods represented by the mapping, $C$, from $\hat{\mathbf{y}}_{t+h|t}$ to $y_{t+h}$, comprises linear, nonlinear, and time-varying combinations. Below we discuss in detail the use of various weighting schemes to determine combination weights.

\subsubsection{Linear combinations}

Typically, the combined forecast is commonly constructed as a linear combination of the individual forecasts. To this end a combined forecast of the linear form can be written as
\begin{align}
\label{eq:linear-combinations}
\tilde{y}_{t+h|t}=\boldsymbol{w}_{t+h|t}^{\prime} \hat{\mathbf{y}}_{t+h|t},
\end{align}
where $\boldsymbol{w}_{t+h|t}=\left(w_{t+h|t, 1}, \ldots, w_{t+h|t, N}\right)^{\prime}$ is an $N$-vector of linear combination weights assigned to $N$ individual forecasts.

\paragraph{Optimal weights}

The seminal work of \cite{Bates1969-yj} proposed a method to find `optimal' weights by minimizing the variance of the combined forecast error, and discussed only the combination of pairs of forecasts. \cite{Newbold1974-lp} then extended the method to the combination of several forecasts. Specifically, assuming that individual forecasts are unbiased and their variance of errors is consistent over time, the combined forecast obtained by a linear combination will also be unbiased. Differentiating with respect to $\boldsymbol{w}_{t+h|t}$ and solving the first order condition, the variance of the combined forecast error is minimized by taking
\begin{align}
\label{eq:weight_opt}
\boldsymbol{w}_{t+h|t}^{\text{opt}}=\frac{\boldsymbol{\Sigma}_{t+h|t}^{-1}\mathbf{1}}{\mathbf{1}^{\prime} \boldsymbol{\Sigma}_{t+h|t}^{-1} \mathbf{1}},
\end{align}
where $\boldsymbol{\Sigma}_{t+h|t}$ is the $N \times N$ covariance matrix of the lead $h$ forecast errors and $\mathbf{1}$ is the $N$-dimensional unit vector. Unfortunately, in practice, the elements of the covariance matrix $\boldsymbol{\Sigma}_{t+h|t}$ are usually unknown and required to be properly estimated.

It follows that if $\boldsymbol{w}_{t+h|t}$ is determined by Equation~\eqref{eq:weight_opt}, one can identify a combined forecast $\tilde{y}_{t+h|t}$ with no greater error variance than the minimum error variance of all individual forecasts. The fact was further demonstrated in detail in \cite{Timmermann2006-en} to illustrate the diversification gains offered by forecast combinations by simply considering the combination of two forecasts. Under mean squared error (MSE) loss, \cite{Timmermann2006-en} characterized the general solution of the optimal linear combination weights given the joint Gaussian distribution of the outcome $y_{t+h}$ and forecasts $\hat{\mathbf{y}}_{t+h|t}$.

The loss assumed in \cite{Bates1969-yj} and \cite{Newbold1974-lp} is quadratic and symmetric in the forecast error from the linear combination. \cite{Elliott2004-dz} examined forecast combinations under more general loss functions that account for asymmetries, and forecast error distributions with skew. They demonstrated that the optimal combination weights in a combination strongly depended on the degree of asymmetry in the loss function and skews in the underlying forecast error distribution. Subsequently, \cite{Patton2007-zo} demonstrated that the properties of optimal forecasts established under MSE loss were not generally robust under more general assumptions about the loss function. The properties of optimal forecasts were also generalized to consider asymmetric loss and nonlinear DGP.

% Optimal weights (A weighted average of forecasts that minimizes the variance of the combined forecast error) -- Unknown covariance matrix -- Five practical methods (relative performance \& time-varying weights).

\paragraph{Regression approach}

The seminal work by \cite{Granger1984-jc} provided an important impetus for approximating the optimal weights under a linear regression framework. They recommended the strategy that the combination weights can be estimated by Ordinary Least Squares (OLS) in regression models having the vector of past observations as the response variable and the matrix of past forecasts as the explanatory variables. Three alternative approaches involving various possible restrictions are considered
\begin{align}
&y_{t+h}=\boldsymbol{w}_{h}^{\prime} \hat{\mathbf{y}}_{t+h|t}+\varepsilon_{t+h}, \quad s.t. \quad \boldsymbol{w}_{h}^{\prime}\mathbf{1}=1, \label{eq:weight_gr1}\\
&y_{t+h}=\boldsymbol{w}^{\prime} \hat{\mathbf{y}}_{t+h|t}+\varepsilon_{t+h}, \label{eq:weight_gr2}\\
&y_{t+h}=\omega_{0h}+\boldsymbol{w}^{\prime} \hat{\mathbf{y}}_{t+h|t}+\varepsilon_{t+h}. \label{eq:weight_gr3}
\end{align}
The constrained OLS estimation of the regression~\eqref{eq:weight_gr1} in which the constant is omitted and the weights are constrained to sum to one yields results identical to the optimal weights proposed by \cite{Bates1969-yj}. Furthermore, \cite{Granger1984-jc} suggested the unrestricted OLS regression~\eqref{eq:weight_gr3} which allowed for a constant term and did not impose the weights sum to one was superior to the popular optimal method regardless of whether the constituent forecasts were biased. However, \cite{De_Menezes2000-vd} put forward some consideration required when using the unrestricted regression, including the stationarity of the series being forecast, the possible presence of serial correlation in forecast errors \citep[see also][]{Diebold1988-sx,Edward_Coulson1993-db}, and the issue of multicollinearity.

More generalizations of the combination regressions have been considered in the literature. \cite{Diebold1988-sx} exploited the serial correlation in least squares framework by characterizing the combined forecast errors as the AutoRegressive Moving Average (ARMA) processes, leading to improved combined forecasts. \cite{Gunter1992-go} and \cite{Aksu1992-lb} provided an empirical analysis to compare the performance of various combination strategies, including the simple average, the unrestricted OLS regression, the restricted OLS regression where the weights were restricted to sum to unity, and the nonnegativity restricted OLS regression where the weights were constrained to be nonnegative. The results revealed that constraining weights to be nonnegative was at least as robust and accurate as the simple average and yielded superiority over other combinations based on regression framework. \cite{Conflitti2015-fq} addressed the problem of determining the optimal weights by imposing two restrictions that the weights should be nonnegative and sum to one, which turned out to be a special case of a lasso regression. \cite{Edward_Coulson1993-db} found that allowing a lagged dependent variable in forecast combination regressions could achieve improved performance. Instead of using the quadratic loss function, \cite{Nowotarski2014-ev} applied the absolute loss function in the unrestricted regression to yield the least absolute deviation regression which was more robust to outliers than OLS combinations.

The forecast combinations using changing weights are developed in the relevant literature to consider various types of structual changes in the constituent forecasts. For instance, \cite{Diebold1987-go} explored the possibilities for time-varying parameters in regression-based combination approaches. Both deterministic and stochastic time-varying parameters are considered in the linear regression framework. Specifically, the combination weights are described as deterministic nonlinear (polynomial) functions of time or allowed to involve random variation. \cite{Deutsch1994-ob} allowed the combination weights to evolve immediately or smoothly using switching regression models and smooth transition regression models.

Researchers have worked on dealing with a large number of forecasts in the regression framework to take advantages of many different models. \cite{Chan1999-io} examined a wide range of combination methods in a Monte Carlo experiment and a real-word dataset. Their results investigated the poor performance of OLS combinations when the number of forecasts to be combined was large and suggested alternative weight estimation methods, such as ridge regression and Principal Component Regression (PCR). \cite{Stock2004-rq} offered the details of principal component forecast combination, which entailed forming a regression having the actual value as the response variable and the first few principal components reduced from several forecasts as the explanatory variables. This method reduces the number of weights that must be estimated in a regression framework, and frequently serves as a way to solve the multicollinearity problem which is likely to lead to unstable behavior in the estimated weights. The superiority of the PCR that involved dimension reduction techniques over OLS combinations was also supproted in \cite{Rapach2008-jh} and \cite{Poncela2011-vz}.

\paragraph{Performance-based weights}

Estimation errors in the optimal weights and a diverse set of regression-based weights tend to be particularly large due to difficulties in properly estimating the entire covariance matrix $\boldsymbol{\Sigma}_{t+h|t}$, especially in situations with the large number of forecasts at hand. Instead, \cite{Bates1969-yj} suggested weighting the constituent forecasts in inverse proportion to their historical performance, ignoring correlations across forecast errors. In follow-up studies, \cite{Newbold1974-lp} and \cite{Winkler1983-ra} generalized the issue in the sense of considering more time series, more forecasting models, and multiple forecast horizons. Their extensive results demonstrated that combinations which took account of correlations performed poorly, and consequently reconfirmed \cite{Bates1969-yj} argument that correlations can be poorly estimated in practice and should be ignored in calculating combination weights.

Let $\mathbf{e}_{t+h|t}=\mathbf{1} y_{t+h}-\hat{\mathbf{y}}_{t+h|t}$ be the $N$-vector of $h$-period forecast errors from the individual models, the five procedures suggested in \cite{Bates1969-yj} for estimating the combination weights when $\boldsymbol{\Sigma}_{t+h|t}$ is unknown, extended to the general case are as follows:
\begin{align}
&w_{t+h|t, i}^{\text{bg1}}=\frac{\left( \sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, i}^{2} \right)^{-1}}{\sum_{j=1}^{N}\left(\sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, j}^{2}\right)^{-1}}. \label{eq:weight_bg1}\\
&\boldsymbol{w}_{t+h|t}^{\text{bg2}}=\frac{\hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1}\mathbf{1}}{\mathbf{1}^{\prime} \hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1} \mathbf{1}}, \quad \text{where} \quad (\hat{\boldsymbol{\Sigma}}_{t+h|t})_{i, j}=\nu^{-1} \sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, i} e_{\tau|\tau-h, j}. \label{eq:weight_bg2}\\
&w_{t+h|t, i}^{\text{bg3}}=\alpha \hat{w}_{t+h-1|t-1, i} + (1-\alpha) \frac{\left( \sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, i}^{2} \right)^{-1}}{\sum_{j=1}^{N}\left(\sum_{\tau=t-\nu+1}^{t} e_{\tau|\tau-h, j}^{2}\right)^{-1}}, \quad 0<\alpha<1. \label{eq:weight_bg3}\\
&w_{t+h|t, i}^{\text{bg4}}=\frac{\left( \sum_{\tau=1}^{t} \gamma^{\tau} e_{\tau|\tau-h, i}^{2} \right)^{-1}}{\sum_{j=1}^{N}\left(\sum_{\tau=1}^{t} \gamma^{\tau} e_{\tau|\tau-h, j}^{2}\right)^{-1}}, \quad \gamma \geq 1. \label{eq:weight_bg4}\\
&\boldsymbol{w}_{t+h|t}^{\text{bg5}}=\frac{\hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1}\mathbf{1}}{\mathbf{1}^{\prime} \hat{\boldsymbol{\Sigma}}_{t+h|t}^{-1} \mathbf{1}}, \quad \text{where} \quad (\hat{\boldsymbol{\Sigma}}_{t+h|t})_{i, j}=\frac{\sum_{\tau=1}^{t} \gamma^{\tau} e_{\tau|\tau-h, i} e_{\tau|\tau-h, j}}{\sum_{\tau=1}^{t} \gamma^{\tau}} \quad \text{and} \quad \gamma \geq 1. \label{eq:weight_bg5}
\end{align}
These weighting schemes differ in the factors, as well as the choice of the parameters, $\nu$, $\alpha$, and $\gamma$. Correlations across forecast errors are either ignored by treating the covariance matrix $\boldsymbol{\Sigma}_{t+h|t}$ as a diagonal matrix or estimated using sample data points which, however, may lead to quite unstable estimates of $\boldsymbol{\Sigma}_{t+h|t}$ given highly correlated forecast errors. Some estimation schemes suggest computing or updating the relative performance of different models over rolling windows of the most recent $\nu$ observations, while others base the weights on exponential discounting with higher values of $\gamma$ giving larger weights to recent observations. In consequence, these weighting schemes are well adapted to allow the non-stationary relationship between the individual forecasting procedures over time \citep{Newbold1974-lp}, which, however, tends to increase the variance of the parameter estimates and works quite poorly provided that the DGP is truly covariance stationary \citep{Timmermann2006-en}.

A broader set of combination weights based on the relative performance of individual forecasting techniques is developed and examined in a series of studies. \cite{Stock1998-np} generalized the rolling window scheme \eqref{eq:weight_bg1} in the sense that the weights on the individual forecasts were inversely proportional to the $k$th power of their MSE. The weights with $k=0$ correspond to assigning equal weights to all forecasts, while more weights are placed on the best performing models by considering $k \geq 1$. Other forms of forecast error measures, such as the Root Mean Squared Error (RMSE) and the symmetric Mean Absolute Percentage Error (sMAPE), are also considered to develop the performance-based combination weights \citep[e.g.,][]{Nowotarski2014-ev,Pawlikowski2020-hm}. Besides, a weighting scheme with the weights depending inversely on the exponentially discounted errors is proposed by \cite{Stock2004-rq} as an upgraded version of the scheme \eqref{eq:weight_bg4}, and encompassed in the sequent studies \citep[e.g.,][]{Clark2010-jx,Genre2013-ut} to achieve gains from combining forecasts. The pseudo out-of-sample performance used in these weighting schemes is commonly computed based on rolling or recursive (expanding) windows \citep[e.g.,][]{Stock1998-np,Clark2010-jx,Genre2013-ut}. It is natural to adopt rolling windows in estimating the weights to deal with the structural change. But the window length should not be too short without the estimates of the weights becoming too noisy \citep{Baumeister2015-ft}.
% rolling windows (min(t-T+1, v)) \citep{Stock1998-np,Clark2010-jx,Genre2013-ut,Baumeister2015-ft,Pawlikowski2020-hm}
% recursive windows (over the past t-T+1 periods) \citep[expanding,][]{Stock1998-np,Stock2003-sp,Stock2004-rq,Clark2010-jx,Genre2013-ut,Nowotarski2014-ev,Baumeister2015-ft}

Compared to constructing the weights directly using historical forecast errors, a new form of combination that is more robust and less sensitive to outliers is introduced based on the `ranking' of models. Again this combination ignores correlations across forecast errors. The simplest and most commonly used method in the class is to use the median forecast as the output. \cite{Aiolfi2006-rh} constructed the weights proportional to the inverse of performance ranks (sorted according to increasing order of forecast errors), which were later used by \cite{Andrawis2011-kb} for tourism demand forecasting. Another weighting scheme that attaches a weight proportional to $\exp (\beta(N+1-i))$ to the $i$th ordered constituent model is adopted in \cite{Yao2008-or} and \cite{Donate2013-lq} to combine Artificial Neural Networks (ANNs), where $\beta$ is a scaling factor. However, as mentioned by \cite{Andrawis2011-kb}, this class of combination method still comes with the drawback of the discrete nature because it limits the weight to only a few possible levels.

\paragraph{Combinations based on information criteria}

Information criteria, such as the Akaike Information Criterion \citep[AIC,][]{Akaike1974-ya}, the corrected Akaike Information Criterion \citep[AICc,][]{Sugiura1978-xm}, and the Bayesian Information Criterion \citep[BIC,][]{Schwarz1978-cz}, are often advised to deal with model selection in forecasting. However, choosing a single model out of the candidate model pool may be misleading because of the loss of information gleaned from alternative models. An alternative way proposed by \cite{Burnham2002-us} is to combine different models based on information criteria to mitigate the risk of selecting a single model.

One such common approach is using Akaike weights. Specifically, in light of the fact that AIC estimates the Kullback-Leibler distance \citep{Kullback1951-hl} between a model and the true DGP, differences in the AIC can be considered to weight different models, providing a measure of the evidence for each model relative to other constituent models. Given $N$ individual models, the Akaike weight of model $i$ can be derived by the following steps:
\begin{align}
&w_{i}^{\text{aic}}=\frac{\exp (-0.5 \Delta \mathrm{AIC}_{i})}{\sum_{k=1}^{N} \exp \left(-0.5 \Delta \mathrm{AIC}_{k}\right)}, \label{eq:weight_aic} \\
&\Delta \mathrm{AIC}_{i}=\mathrm{AIC}_{i}-\min _{k \in \{1,2,\cdots,N\}} \mathrm{AIC}(k). \nonumber
\end{align}
Akaike weights calculated in this manner can be interpreted as the probability that a given model performs best at approximating the unknown DGP, given the model set and data \citep{Kolassa2011-ai}. Similar weights from AICc, BIC, and other variants with different penalties, can be derived analogously to Equation~\eqref{eq:weight_aic}.

The outstanding performance of weighted combinations based on information criteria has been confirmed in several research. For instance, \cite{Kolassa2011-ai} used weights derived from AIC, AICc and BIC to combine exponential smoothing forecasts, and resulted in superior accuracy over selection using these information criteria. The similar strategy was adopted by \cite{Petropoulos2018-fw} to separately explore the benefits of bagging for time series forecasting. Furthermore, an empirical study by \cite{Petropoulos2018-ad} showed that a weighted combination based on AIC improved the performance of the statistical benchmark they used.

% the use of multiple temporal aggregation levels

\paragraph{Bayesian approach}

Some effort has been directed toward the use of Bayesian approaches to updating forecast combination weights in face of new information from various sources. Recall that obtaining reliable estimates of the covariance matrix $\boldsymbol{\Sigma}$ (the time and horizon subscripts are dropped for simplicity) of forecast errors with correlation being ignored or not, is a major challenge in the general case. With this in mind, \cite{Bunn1975-vz} suggested the idea of a Bayesian combination on the basis of the probability of respective forecasting model performing the best on any given occasion. Considering the beta and the Dirichlet distributions arising as the conjugate priors for the binomial and multinomial processes respectively, the suggested non-parametric method performs well when there is relatively little past data by the way of attaching prior subjective probabilities to individual forecasts \citep{Bunn1985-vo,De_Menezes2000-vd}. \cite{Oller1978-wx} presented another way to involve subjective probability in a Bayesian updating scheme based on the self-scoring weights proportional to the evaluation of the expert's forecasting ability.

A different theme of research has also advocated the incorporation of prior information into the estimation of combination weights, but with the weights being shrunk toward some prior mean under a regression-based combination framework \citep{Newbold2002-wa}. Assuming that the vector of forecast errors is normally distributed, \cite{Clemen1986-pd} developed a Bayesian approach with the conjugate prior for $\boldsymbol{\Sigma}$, represented by an inverted Wishart distribution with covariance matrix $\boldsymbol{\Sigma}_{0}$ and scalar degrees of freedom $\nu_{0}$. Again we drop time and horizon subscripts for simplicity. If the last $n$ observations are used to estimate $\boldsymbol{\Sigma}$, the combination weights derived from the posterior distribution for $\boldsymbol{\Sigma}$ is
\begin{align}
\label{eq:weight_cw}
\boldsymbol{w}^{\text{cw}}=\frac{\boldsymbol{\Sigma}^{*-1}\mathbf{1}}{\mathbf{1}^{\prime} \boldsymbol{\Sigma}^{*-1} \mathbf{1}},
\end{align}
where $\boldsymbol{\Sigma}^{*}=\left[\left(\nu_{0} \boldsymbol{\Sigma}_{0}^{-1}+n \hat{\boldsymbol{\Sigma}}^{-1}\right) /(\nu_{0}+n)\right]^{-1}$ and $\hat{\boldsymbol{\Sigma}}$ is the sample covariance matrix.
Compared to estimating $\boldsymbol{\Sigma}$ using past data or treating it as a diagonal matrix, the proposed approach yields superiority, in terms of providing a relatively stable estimation and allowing correlations by specifying the prior estimate $\boldsymbol{\Sigma}_{0}$. The subsequent work by \cite{Diebold1990-fk} allowed the incorporation of the standard normal-gamma conjugate prior by considering a normal regression-based combination
\begin{align}
\label{eq:weight_dp}
\mathbf{y}=\hat{\mathbf{Y}} \boldsymbol{w}+\boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim N\left(\mathbf{0}, \boldsymbol{\sigma}^{2} \mathbf{I}\right),
\end{align}
where $\mathbf{y}$ and $\boldsymbol{\varepsilon}$ are $T$-dimensional vectors, and $\hat{\mathbf{Y}}$ is the $T \times N$ matrix of constituent forecasts. The approach results in estimated combination weights which can be considered as a matrix weighted average of those for the two polar cases, least squares and prior weights. It can provide a rational transition between subjective and data-based estimation of the combination weights. In light of the fact that Bayesian approaches have been mostly employed to construct combinations of probability forecasts, we will elaborate on other newly developed methods of determining combination weights in a Bayesian context in the following Section \ref{sec:comb_prob}.

% \cite{Bunn1981-lk} `veridical' probabilities (probabilities that specific models represent the true process)
% \citep{Cheng2015-tp,Nowotarski2014-ev}.

\subsubsection{Nonlinear combinations}
% Evidently, a substantial body of literature has accumulated with linear combination methods (both simple and weighted linear combinations), while very limited attention has been placed on nonlinear approaches, so far.
So far our attention has been placed on linear combination schemes, including simple and weighted methods. Linear combination approaches implicitly assume a linear dependence between constituent forecasting models and the variable being forecasted \citep{Donaldson1996-um,Freitas2006-fn}, which may result in the optimal combination at a particular point but not the best forecast \citep{Ming_Shi1999-vs}. Therefore, the trustworthy of linear combinations may be quite questionable if the individual forecasts come from nonlinear models or if the true relationship between combination members and the best forecast is characterized by nonlinear systems \citep{Babikir2016-xz}. In such cases, it follows naturally to relax the linearity assumption and consider nonlinear combination schemes which, however, have received very limited research attention so far.

As classified by \cite{Timmermann2006-en}, two types of non-linearities can be considered in forecast combinations. The first type involves nonlinear functions of the individual forecasts, but with the unknown parameters of combination weights being in linear form. While the second method allows a more general combination where non-linearities are considered in the combination parameters. However, considerable estimation errors may be produced in such cases and thus may require additional numerical efforts.

\cite{Shanming_Shi1993-uk} suggested the use of the neural networks for nonlinear combination by demonstrating that it served as an effective method to solve the problem of forecasting the price of IBM stock. Since then, a stream of studies has been devoted to conduct nonlinear combination of forecasts using the neural networks. \cite{Donaldson1996-um} used Artificial Neural Networks (ANNs) to obtain the combined forecasts $\tilde{y}_{t+h|t}$ by the following form
\begin{align}
\tilde{y}_{t+h|t} &=\beta_{0}+\sum_{j=1}^{k} \beta_{j} \hat{y}_{t+h|t, j}+\sum_{i=1}^{p} \delta_{i} g\left(\mathbf{z}_{t+h|t} \mathbf{\gamma}_{i}\right), \label{eq:nonlinear_sl}\\
g\left(\mathbf{z}_{t+h|t} \mathbf{\gamma}_{i}\right) &=\left(1+\exp \left(-\left(\gamma_{0, i}+\sum_{j=1}^{N} \gamma_{1, j} z_{t+h|t, j}\right)\right)^{-1}\right., \nonumber \\
z_{t+h|t, j} &=\left(\hat{y}_{t+h|t, j}-\bar{y}_{t+h|t}\right) / \hat{\sigma}_{y t+h|t}, \nonumber \\
k & \in \{0,N\} \quad p \in \{0,1,2,3\}, \nonumber
\end{align}
where $\bar{y}_{t+h|t}$ and $\hat{\sigma}_{y t+h|t}$ were the in-sample mean and standard deviation across the forecasting models respectively using data up to time $t$. This scheme permitted special cases of both purely linear combination ($k=N, p=0$) and nonlinear combination ($k=0, p\neq 0$). Following them came an extension work by \cite{Harrald1997-gd} to evolve ANNs and demonstrate its utility when only forecasts to combine were given. \cite{Krasnopolsky2012-xu} and \cite{Babikir2016-xz} employed neural network approaches with various activation functions to approximate the nonlinear dependence of individual forecasts and achieve the nonlinear mapping, which were essentially variants of Equation~\eqref{eq:nonlinear_sl}. The empirical results of nonlinear combination from these studies offered forecasts which generally dominated forecasts from traditional linear combining procedures, such as simple average, OLS weights, and performance-based weights. The superiority arose possibly due to the nueral networks' outstanding learning ability and flexibility to uncover hidden nonlinear relationships not easily captured by traditional linear combinations \citep{Donaldson1996-um,Babikir2016-xz}. However, these empirical evidence for the gains from incorporating nonlinear combinations was given based on less than ten time series (mainly economic or financial data), and thus was not statistically significant and probably produced by the dodgy selection of data. Besides, these nonlinear combination methods suffered from other drawbacks, including the neglect of correlation in forecast errors, the instability of parameter estimation, and the multicollinearity issue cased by the overlap in the information sets used to produce the individual forecasts. The superiority of nonlinear combinations over linear combining schemes, thus, need to be further investigated.

Other authors sought to construct nonlinear combinations via including a nonlinear term to cope with the case where the forecast errors from constituent models were correlated. This combination mechanism can be generalized to the following form
\begin{align}
\label{eq:nonlinear_corr}
\tilde{y}_{t+h|t} =\beta_{0}+\sum_{j=1}^{N} \beta_{j} \hat{y}_{t+h|t, j}+\sum_{i,j=1 \atop i<j}^{N}\pi_{ij}v_{i j}.
\end{align}
By this way, the usual framework for linear combinations is extended. The involved nonlinear terms take various definitions. For example, \cite{Freitas2006-fn} intuitively defined $v_{i j}$ as the product of individual forecasts from different models, $\hat{y}_{t+h|t, i} \cdot \hat{y}_{t+h|t, j}$, while \cite{Adhikari2012-ur} took into account the linear correlations among the forecast pairs by including the term, $(\hat{y}_{t+h|t, i}-\bar{y}_{i})(\hat{y}_{t+h|t, j}-\bar{y}_{j})/(\sigma_{i}\sigma_{j})^2$, where $\bar{y}_{i}$ and $\sigma_{i}$ were the mean and standard deviation of the $i$th model. Moreover, \cite{Adhikari2015-bb} defined the nonlinear term using $\left(\hat{z}_{i}-m_{i j} \hat{z}_{j}\right)\left(\hat{z}_{j}-m_{j i} \hat{z}_{i}\right)$, where the term $m_{i j}$ was the degree of mutual dependency between the $i$th and $j$th forecasting model, and $\hat{z}_{i}$ was the standardized individual forecasts using the mean $\bar{y}_{i}$ and standard deviation $\sigma_{i}$. Five correlation measures were investigated for measuring the mutual associations between two different forecasts.

Clearly, the area of considering non-linearities in forecast combinations requires further research. First, the forecasting performance of nonlinear combination schemes should be further examined for a large, diverse collection of time series datasets as well as with some statistical inference. Second, the high correlation across forecast errors as well as the multicollinearity issue has to be addressed carefully in the framework of nonlinear combinations. For example, the PCR model can be considered as an alternative approach to dealing with such concerns by projecting the individual forecasts onto a smaller subspace via principal component analysis.

\subsubsection{Combining by learning}

Stacking \citep[stacked generalization,][]{Wolpert1992-if} provides a strategy to adaptively combine the available forecasting models by learning from a single model. While stacking is frequently employed on a wide variety of classification tasks \citep{Zhou2012-cy}, it introduces the concept of meta-learning in the context of time series forecasting with the purpose of boosting forecasting accuracy beyond the level achieved by any of the individual models. On one hand, stacking is a general framework which consists of at least two levels. Considering a stacking approach involving two levels. The first level entails the training of individual forecasting models using the original data, while the model, too called meta-model, is learned in the second level using the first-level forecasts as attributes to form a final set of forecasts. Note that the meta-model in a certain level must use the forecasts of models in the previous level as inputs. On the other hand, stacking can be regarded as a generic combination method which combines by learning. In this respect, we introduce stacking approach here as a class of combination methods, which adaptively weighs individual forecasts using meta-learning processes.

There are many different ways to implement the stacking strategy. Its primary implementation is as a technique for combining individual models in a series-by-series fashion. Simply put, individual forecasting models in the method pool are trained using only data of the single series they are going to forecast, their forecast outputs are subsequently fed to a meta-model tailored for the target series to calculate the combined forecasts. This means that $n$ meta-models are required for $n$ different series. Unsurprisingly, OLS regression \citep[e.g.,][]{Granger1984-jc,Gunter1992-go} falls into this category and can be viewed as the most simple, common learning algorithm used in stacking. Instead of applying multiple linear regression, \cite{Moon2020-ls} suggested the PCR model as the meta-model predominantly due to its desirable characteristics such as dimensionality reduction, and avoidance of multicollinearity between the input forecasts of individual models. Similarly, lasso regression, as well as machine learning techniques, such as ANN, Wavelet Neural Network (WNN), and Support Vector Regression (SVR) can therefore be conducted in a series-by-series fashion to combine constituent models \citep[e.g.,][]{Donaldson1996-um,Conflitti2015-fq,Ribeiro2019-wk,Ribeiro2020-mj}. One could consider the use of the expanding or rolling window method to ensure that enough individual forecasts are generated for the training of meta-models. Time series cross-validation, also known as ``evaluation on a rolling forecasting origin", is also recommended in the training procedures for both individual models and meta-models to help with the parameters determination. Nevertheless, stacking approaches implemented in a series-by-series fashion still suffer from some limitations such as time wasting, high requirements for time series length, and insufficiency of training data for meta-models.

An alternative way to implement the stacking strategy sheds some light on the potential of cross-learning. Specifically, the meta-model is trained using information derived from multiple series without focusing only on a single series, thus various patterns can be captured along different series. The M4 competition \citep{Makridakis2020-hu}, comprising $100,000$ time series, recognized the benefits of cross-learning in the sense that the top three performing methods of the competition utilized the information across the whole dataset rather than a single series. Cross-learning can therefore be identified as a promising strategy to boost forecasting accuracy, at least when appropriate strategies for extracting information from large, diverse time series datasets are considered \citep{Kang2020-sa,Semenoglou2020-xx}. \cite{Zhao2020-ep} trained a neural network model across the M4 competition dataset to learn how to combine different models in the method pool. They adopted the temporal holdout strategy to generate the training dataset and utilized only the out-of-sample forecasts produced by standard individual models as the input in the neural network model.

An increasing stream of studies has shown that time series features, additional inputs describing each series in a dataset, provide valuable information for forecast combination in a cross-learning fashion, leading to an extension of stacking. The seminal work by \cite{Collopy1992-ey} developed a rule base consisting of $99$ rules to combine forecasts from four statistical models using $18$ time series features. \cite{Petropoulos2014-uy} identified the main determinants of forecasting accuracy through a empirical study involving $14$ forecasting models and seven time series features. The findings can provide useful information for forecast combination. More recently, \cite{Montero-Manso2020-tq} introduced a feature-based approach, which employed $42$ statistical features \citep[implemented using the R package tsfeatures,][]{rtsfeatures} to estimate the optimal weights for combining nine different traditional models trained per series based on an XGBoost model. The method reported the second-best forecasting accuracy in M4 competition. \cite{Li2020-od} extracted time series features automatically with the idea of time series imaging, then these features were used for forecast combination.

In addition to the time series features extracted from the historical data, it is crucial to look at the diversity of the individual model pool in the context of forecast combination \citep{Jose2014-uh,Lichtendahl2020-ut}. An increase in diversity among forecasting models can improve the accuracy of their combination. In this respect, features describing diversity of the method pool should be included in the feature pool to provide additional information possibly relevant to combining models. \cite{Lemke2010-wn} calculated six diversity features and created an extensive feature pool describing both the time series and the individual method pool. Three meta-learning algorithms were implemented to link knowledge on the performance of individual models to the features, and to improve forecasting performance. \cite{Kang2020-sv} utilized a group of features only measuring the diversity across the candidate forecasts to construct a forecast combination model mapping the diversity matrix to the forecast errors. The proposed approach yielded comparable forecasting performance with the top-performing methods in the M4 competition.

As expected, the implementations of stacking in a cross-learning manner also come with their own limitations. The first limitation is the requirement for a large, diverse time series dataset to enable meaningful training. This issue can be addressed by simulating series on the basis of the assumed DGPs \citep{talagala2018meta}, which are exponential smoothing models and ARIMA models, and by generating time series with diverse and controllable characteristics \citep{Kang2020-rl}. Moreover, given considerable literature on feature identification and feature engineering \citep[e.g.,][]{Wang2009-hs,Kang2017-wt,Lemke2010-wn,Montero-Manso2020-tq,Li2020-od}, the feature-based forecast combination methods naturally raise the concern about how to design an appropriate feature pool in order to achieve the best out of such methods. Other major limitations lie with the design of loss function for the meta-model, and the requirement of significant training time.

\subsubsection{Which forecasts should be combined?}

forecast pooling / optimal forecast groups / model selection
\citep{Zhou2002-cg,Kourentzes2019-na}



\subsection{Forecast Combination Puzzle}
{\color{red} (why don't weights work?)}

\citep{Kang1986-kq,De_Menezes2000-vd,Genre2013-ut,Post2019-lv,Chan2018-jl,Lichtendahl2020-ut,Kourentzes2019-na}

\cite{Diebold1990-fk} uses Bayesian shrinkage techniques to allow the incorporation of prior information into the estimation of combining weights; least squares and prior weights then emerge as polar cases for the posterior mean.
% About "forecast combination puzzle"
% This empirical fact has been first stated as the ``forecast combination puzzle" in \cite{Stock2004-rq}. Furthermore, some theoretical explanation of the forecast combination puzzle are offered in later studies \citep[see][]{Smith2009-wd,Claeskens2016-pv}.
% 1. This puzzle was first noted by Clemen (1989), and was formally named the forecast combination puzzle by Stock and Watson (2004).
% 2. In Makridakis et al., (1982, 1983), a simple average of six methods tended to be slightly more accurate than a weighted average of the same methods.


\section{Probabilistic forecast combinations}
\label{sec:comb_prob}
\begin{itemize}
\item Combining quantiles and prediction intervals.
\item Combining distributions as in fable.
\end{itemize}

\citep{Genest1990-sr,Nowotarski2015-xu,Conflitti2015-fq,Billio2013-sg}

Bayesian analysis
\citep{Winkler1968-uw,Clemen1999-mh}

BMA
\citep{Li2011-nw}

% About "simple probabilistic forecast combination"
% 1. There has been a growing interest in using combination methods for probabilistic forecasting. Probabilistic forecasts differ from point forecasts in the sense that there is an associated probability related to the reported values, providing more comprehensive information about uncertainties of the future load than single-valued forecasts. 
% 2. histories (theory and empirical work of simple combination)
%% References:
%% - Is It Better to Average Probabilities or Quantiles?
%% - The opinion pool (and its citations)
%% - Combining probability distributions from experts in risk analysis
%% - Combining probability distributions: A critique and an annotated bibliography
% 3. three types and some basic combination methods
%% References:
%% - (interval)Combining Prediction Intervals in the M4 Competition
%% - Combining Interval Forecasts.
%% - (quantile)Combining Probabilistic Load Forecasts

\section{Probabilistic ensembles}
\begin{itemize}
\item Meteorological ensembles.
\item True ensembles in other areas (i.e., not papers that use the word "ensemble" but papers that use mixtures when forecasting).
\item When is an ensemble equivalent to combination?
\item When do point forecasts from an ensemble equal point forecasts from a combination?
\end{itemize}

\section{Boosting in forecasting}
% Hybrid systems \citep{Zhang2003-go,Pai2005-ea,Yu2005-px,De_Mattos_Neto2017-iz,Chen2021-lx}
\section{Bagging in forecasting}
\section{Stacking in forecasting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% References %%%%%%%%%%%%%%
\bibliographystyle{agsm}
\bibliography{references.bib}

\end{document}
